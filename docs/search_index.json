[
["index.html", "R Training Resources and Tips Introduction", " R Training Resources and Tips Alan Yeung 03 May 2020 Introduction This is a list of R resources, links and tips that is being curated to help particularly with training on the Tidyverse and working efficiently in R. A good place to get started with learning about R is the PHI Introduction to R in NSS. If you want to try out R without installing it, you can do this on RStudio Cloud. For the tidyverse section, I provide a recommended order to work through materials for training. However, it is, of course, not necessary to follow this exactly – particularly if you feel you are already comfortable with some of the material. I leave it to the reader/participant to use this as they see fit. I am very grateful to Sean Kross for providing a simple-to-use bookdown template to get me started, and of course, Yihui Xie for creating the fantastic bookdown package. Finally, this is a work-in-progress and will need updating over time. Therefore, if you have suggestions, comments or feedback, please get in touch. "],
["getting-started.html", "Chapter 1 Getting Started 1.1 Understanding R from the Perspective of Excel 1.2 Setting Up RStudio 1.3 Getting Help", " Chapter 1 Getting Started 1.1 Understanding R from the Perspective of Excel This may be useful reading if you have never used R (or similar software for data analysis) before but are familiar with Microsoft Excel. R for Excel users Where do things live in R? R for Excel Users What’s a variable in R? A Guide to R for Excel Users 1.2 Setting Up RStudio When you first load RStudio, you should set up some options within it. First go to the Tools menu and then go into the Global Options. In the General tab, you should untick ‘Restore .RData into workspace at startup’ and change ‘Save workspace to .RData on exit’ to ‘Never’. This is illustrated in the Workflow: projects chapter in the R for Data Science book. Doing this helps to ensure that R loads in a fresh session every time which also helps to ensure your work is fully reproducible. In addition to this, you should make sure R is set up to work with the unicode character set in case you ever need to use them. To do this, go to the Code tab (within Global Options) and then go to the Saving tab within there and make sure the ‘default text encoding’ is ‘UTF-8’. Some optional settings: A lot of programmers like to use a dark theme for coding so if this is your preference, go to the Appearance tab and set the ‘Editor theme’ to something like ‘Twilight’. If you are working in the UK, go the Spelling tab, and set the ‘Main dictionary language’ to ‘English (United Kingdom)’. 1.3 Getting Help To use R effectively, it is more or less essential to learn how to use the help facilities within R. This is because there are just so many functions available (especially when you consider the amount of packages that have been developed to extend the capabilities of R) and it is difficult to always remember exactly how to use them. Help pages for functions in R can be brought up by typing ?name-of-function within R, e.g. to get the help for the mean() function, you can type ?mean. Equally, help(name-of-function) also works, e.g. help(mean). The help pages in R can be a little daunting at first so it is good to start by understanding the layout of a help page and what everything in it means. How to read a help page in R Analysis essentials: Using the help page for a function in R In addition to the help pages within R, there are also many other ways to get help. Where to get help with your R question? You can search online for help in your favourite search engine. Often typing something like ‘joining data in R’ will be good enough to return useful results. There is also a search page focussed on returning R-related results called RSeek if you want to try that. People are usually good at responding to questions if you post them online in a reproducible manner. Here is a good guide for doing this well using the reprex package in R. So you’ve been asked to make a reprex You can post your question to the Data Science Scotland Slack R Channel or to websites such as StackOverflow. "],
["tidyverse.html", "Chapter 2 Tidyverse 2.1 Installation 2.2 Getting Data into R 2.3 Data Manipulation with dplyr 2.4 Reshaping Data with tidyr 2.5 Other Data Manipulation Tips 2.6 Further Resources", " Chapter 2 Tidyverse This statement from the official tidyverse website describes what the tidyverse is The tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures. Further background information on what the tidyverse is can be found in the R Views blog: What is the tidyverse?. In addition, Hadley Wickham has laid out his principles for tidyverse packages within his tidy tools manifesto. At this point, you should watch this video to get an introduction to data wrangling with the tidyverse. What is data wrangling? Intro, Motivation, Outline, Setup – Pt. 1 Data Wrangling Introduction 2.1 Installation The package can be installed and loaded with install.packages(&quot;tidyverse&quot;) library(tidyverse) This loads the core set of tidyverse packages – see here for more details. 2.2 Getting Data into R The tidyverse installs a number of packages that help with getting your data into R. However, only the readr package is loaded as part of the core set of packages so the other data import packages, (readxl and haven have to be loaded with their own library() calls). readr: for reading in data formats such as csv, tsv and fwf. readxl: for reading in Excel data formats (.xls and .xlsx). haven: for reading in SPSS, Stata and SAS data formats. RStudio also provides an ‘Import Dataset’ button to help with this. The RStudio support pages describe how to import data with RStudio. If you decide to use this to help you import data into R, my recommendation is to copy over the code generated in the ‘Code Preview’ box into your R script. This helps you to keep a record of the data import step in your script. Besides these data types, you should familiarise yourself with R’s own data formats: .rds and .RData – .rds format can be used when saving a single object or dataset while .RData should only be used when you want to save multiple objects or datasets from your R session. These formats load fast into R and an important advantage is that they keep data exactly as they were when they were created in R (i.e. all information about variable types and metadata will be retained after saving). .rds: read in data with readRDS(); save data with saveRDS() .RData: read in data with load(); save data with save() 2.3 Data Manipulation with dplyr dplyr provides a set of functions (often described as verbs) to help with data manipulation. Most common tasks can be done using only a handful of functions: select(), filter(), mutate(), rename(), arrange(), summarise() and group_by() – note that rename() is not usually included in this list but I have included it as renaming variables comes up a lot during data cleaning/processing. Out of these functions, group_by() is the most complex and should probably be tackled after understanding the others; in particular, it is often used together with summarise(). Furthermore, two functions that often come in handy when used together with mutate() are ifelse() (or the stricter dplyr equivalent if_else()) and case_when() – these help to create new variables depending on certain conditions being met. It is a good idea to understand what each of these functions does on their own first before thinking about how to combine multiple manipulation steps together using pipes (%&gt;%). Once you are comfortable with each of these functions, it should be a relatively small step to learn how to combine them together with the pipe operator; the ability to do this in a way that is relatively intuitive is one of the most powerful design features of the tidyverse. This is more or less the learning order used in the dplyr chapter of the R Programming for Data Science book which is worth a read through (there is also an accompanying video for that chapter which may also help). I would definitely recommend spending a good amount of time learning dplyr as you will want to reach the point where you can use these functions without requiring much thinking. A good place to get started on learning about these functions are these videos. Data Manipulation Tools: dplyr – Pt 3 Intro to the Grammar of Data Manipulation with R Hands-on dplyr tutorial for faster data manipulation in R – note that this tutorial is a little old (2014) so some parts of it are out-of-date but I think it is still useful. Only minor changes will be needed to make this current and the old dplyr functions mentioned should still be useable anyway. For example, they use tbl_df() which can now be replaced by tibble() instead. You should try running the R commands yourself while watching this video to aid the learning process. Once you have watched these videos, you can work through all the tutorials in the Work with Data section on RStudio Cloud. Then you can further consolidate this knowledge by reading through data transformation chapter of the R for Data Science book and working through the exercises at the end. After this, a set of four tutorials by Suzan Baert provides much more depth on what some of the basic dplyr functions can do. Again, trying to run the R codes while following along with the tutorials will be very beneficial. Data Wrangling Part 1: Basic to Advanced Ways to Select Columns Data Wrangling Part 2: Transforming your columns into the right shape Data Wrangling Part 3: Basic and more advanced ways to filter rows Data Wrangling Part 4: Summarizing and slicing your data 2.3.1 Working with Multiple Datasets It is very likely that you will need to perform data linkage at some point so this makes it more or less essential to learn how to work with the various joining functions within dplyr. To get a quick overview on this, you can watch this short video. Working with Two Datasets: Binds, Set Operations, and Joins – Pt 4 Intro to Data Manipulation These neat animations then offer a great illustration of how these joins work and a cheatsheet by stat545 summarises this further. After this, I recommend reading three short sections of the relational data chapter in the R for Data Science book: Understanding joins, Inner join, Outer joins. Then you can follow this video tutorial – the parts about joins start from around 25 minutes into the video but if you want to further solidify your dplyr knowledge, you can watch it from the beginning. Again, note that this video is a little old (2015) so some aspects may be out-of-date but it is still useful nevertheless. Going deeper with dplyr: New features in 0.3 and 0.4 (tutorial) There is also a Join Data Sets tutorial on RStudio Cloud that you can try. Finally, you will want to read and work through all of relational data chapter in the R for Data Science book to understand how to work with multiple related datasets more thoroughly. By the end, you should be familiar with just about everything on this Data Transformation Cheat Sheet. 2.4 Reshaping Data with tidyr Sometimes you will need to format your data into a specific shape to make it work for a certain purpose – tidyr is a package that has functions that help with this. For example, for output tables, you may want to have data on different years in separate columns while for analytical operations within R, it may be easier to work with ‘year’ stored in only one column (or variable); the latter format here can be considered as tidy data. In particular, the ggplot2 package often requires data to be in the long or tidy format. To get a better idea on this, watch this video. Tidy Data and tidyr – Pt 2 Intro to Data Wrangling with R and the Tidyverse – however please note that the gather() and spread() functions mentioned will be replaced by pivoting functions so you should mainly be concerned with the concepts here rather than learning about these functions specifically. Then you can try the Reshape Data tutorial on RStudio Cloud. The main thing you need to be able to do with tidyr is to be confident in knowing how to transform your data from wide-to-long format or from long-to-wide format. However, if you want to know more about what tidyr can do or find out more about tidy data in general, please refer to the tidy data chapter in the R for Data Science book (keeping in mind the caveat made previously about the change to using pivoting functions). 2.5 Other Data Manipulation Tips As you progress further along with learning to manipulate data using the tidyverse, you will inevitably encounter situations where you need to know more about how to deal with specific types of data such as dates/times and string variables. In addition, there will also be times when you wish there was a convenient and quick way to process data – luckily there usually is in R. Please note that some of the material mentioned in this section is probably optional until you specifically have a need for it but I recommend that you go through some of it anyway to build an awareness of where to look for solutions when you need it. 2.5.1 Dates/Times The main package within the tidyverse for dealing with dates and times is the lubridate package. This is not loaded with the core set of tidyverse packages so has to be loaded separately by typing library(lubridate). The main reference text for learning about how to work with dates/times using lubridate is the dates and times chapter in R for Data Science. Dates/times can sometimes be very complex (e.g. when thinking about time calculations across timezones) but it is likely that you will usually only need to perform fairly simple data processing operations involving dates. In this case, you will only really need to know a small number of functions from lubridate: dmy(), mdy(), ymd() to convert text to a Date format in R, e.g. ymd(\"2001/05/20\") would convert the text “2001/05/20” so that R can recognise this as 20 May 2001. year(), month(), day() to extract the year, month, day from a date object, e.g. year(ymd(\"2001/05/20\")) would extract 2001. 2.5.2 Strings The stringr package package is loaded within the core set of tidyverse packages and provides a set of consistent functions for working with string (or text) data. Again, the main reference is the strings chapter in R for Data Science. You will likely not need to know everything contained in that chapter but I would recommend learning some basics of using regular expressions. These can help to, for instance, filter data or create new variables according to some text pattern found in a string variable. This can sometimes be much quicker than filtering using a list of keywords, particularly if your data may contain variations on a common item (e.g. ‘Male’, ‘M’, ‘Man’) or mispellings (e.g. ‘Mal’). A useful place to practice and learn how to use regular expressions is regexr. There is also an RStudio addin that may help called regexplain which was inspired by regexr but may help with your learning as it can be used within RStudio. Once you gain some familiarity on how to use regular expressions, many of the stringr functions should be relatively easy to understand and use. In particular, str_detect() is useful for filtering and creating variables. If you are already familiar with regular expressions using base R functions and want to find out about their stringr equivalents to take advantage of the consistencies in their design, the from base R to stringr vignette is a handy resource. Furthermore, this blog post on Demystifying Regular Expressions in R gives a very clear and descriptive guide to how the different base R regular expression functions work. 2.5.3 janitor The janitor package is a R package that has simple functions for examining and cleaning dirty data. It was built with beginning and intermediate R users in mind and is optimised for user-friendliness. A really neat function from janitor is clean_names(). This function has various different options for cleaning variables names but the defaults usually work quite well – this will transform variable names into lower-case and snake-case. We can use the iris dataset to see how this looks (note that :: can be used before a function name to specify the package that the function comes from). names(iris) ## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; &quot;Species&quot; names(janitor::clean_names(iris)) ## [1] &quot;sepal_length&quot; &quot;sepal_width&quot; &quot;petal_length&quot; &quot;petal_width&quot; &quot;species&quot; This variable name cleaning can also be applied at the same time as reading in a data file. For example, when reading in an Excel file, you can use something like readxl::read_excel(\"excel_file\", .name_repair = janitor::make_clean_names). Another function from janitor that is quite useful is tabyl(). This creates tables of counts, including cross-tabulations and outputs to dataframe format – this is the main benefit over using something like table() from base R as dataframes are often easier to manipulate, particularly with the help of the myriad tidyverse functions. janitor::tabyl(mtcars, cyl, gear) ## cyl 3 4 5 ## 4 1 8 2 ## 6 2 4 1 ## 8 12 0 2 2.6 Further Resources R for Data Science by Garrett Grolemund and Hadley Wickham R for Data Science: Exercise Solutions by Jeffrey B. Arnold Yet another ‘R for Data Science’ study guide by Bryan Shalloway YaRrr! The Pirate’s Guide to R "],
["efficiency.html", "Chapter 3 Efficiency 3.1 Extracting Data from Databases 3.2 Processing 3.3 Importing and Exporting Data 3.4 Further Resources", " Chapter 3 Efficiency As the amount of data being captured and stored continues to grow at ever increasing speed, it becomes more and more crucial to learn how to work efficiently using the tools at your disposal. Regardless of whether or not you are working with big data, it is important to write code that is efficient anyway. Working on analysis projects in R is often fluid and evolves dynamically according to feedback gained while exploring your data. Therefore if you work efficiently, you can get this feedback quicker and iterate through analyses more rapidly. Another aspect to efficiency is that you may not always be working with R from a powerful machine and in this case, it becomes essential to work efficiently. In addition, if many users are carrying out their work by connecting to e.g. an RStudio Server concurrently, then the server has to distribute resource to all of these users. If some users are working very inefficiently (e.g. by extracting huge amounts of data or running highly intensive processing operations) then this can cause strain on the server which can have consequences for all users that need to use the server. This can be mitigated by setting up limitations on how much server resource users can be allocated. However, it is nevertheless, important that users all try to work efficiently in order to ensure that things continue to operate smoothly. 3.1 Extracting Data from Databases If an organisation holds large amounts of data in databases that lots of users need to download data from then it is important for users to work with these databases in an efficient manner. The key question to ask yourself whenever you want to extract data from the database is What is the minimum amount of data I need to solve the problem? Within this, there are some further sub-questions you should ask yourself every time. What variables do I need to solve the problem? What variables do I not need? In SQL, this would be taken care of using SELECT and listing the variables you want to download. For these variables, do I actually need all of the records in a database to solve my problem or just a subset of records? For example, do I just need records covering a particular time period? In SQL, this part is usually taken care of using WHERE and specifying filtering conditions. If you think about just these questions carefully enough before extracting data, often the amount of data you actually download into R will be much smaller compared with extracting an entire database into R (particularly if the database is very large). Working with this smaller amount of data can often already result in large efficiency gains from both a memory-management and processing perspective (less data gets loaded into R and performing data processing on smaller objects can be much faster). You can go further than this and think about whether or not you can aggregate data before bringing it into R. This is a particularly good idea when extracting non-aggregated records would still result in extracting huge amounts of data (even after carefully considering some the questions mentioned above). In SQL, this is usually done using GROUP BY, combined with something like COUNT. 3.1.1 Extracting Data from SMRA For those working within the NHS in Scotland, Emily Moore has written a paper about Using R with SMRA (SMRA is a large relational database containing Scottish morbidity records). This paper gives detail about how to connect to the database using RStudio and also provides guidance on making efficient queries using SQL within R. 3.1.2 dbplyr If you have the odbc package set up to connect to databases, then you can use dbplyr to help you write efficient SQL queries using dplyr code. The biggest benefit here is for people who have learned how to manipulate data using dplyr but have not learned SQL. It also provides other advantages such as being able to take advantage of helper functions within dplyr (e.g. starts_with() to select variables) to make the amount of code you need to write more concise compared with SQL. The main idea of dbplyr is to write dplyr code as if you are just working with data that is in R already (as opposed to working with data stored in a database) but at the end, you have to add a step to actually download the data from the database into R using collect(). At it’s core, all dbplyr does is translate code written using dplyr functions into SQL. Therefore, if you are interested in seeing the SQL code it generates, you can do so by running show_query(). dbplyr will likely not be able to provide all of the functionality available in SQL (at least not without inserting bits of actual SQL code in with dbplyr code) but it should be able to do what you need the majority of the time. Bob Taylor has written a short tutorial on how to use dbplyr to extract data from SMRA. Some of this is based on a cheatsheet of SQL-SPSS queries produced by Jaime Villacampa. Encouragingly, speed tests run in the tutorial also shows that dbplyr is fast so this is a great bonus. 3.1.3 Temporary Objects Remove temporary objects as you go along in an analysis, particularly if those objects take up a lot of memory (e.g. temporary large dataframes). The objectremover RStudio addin may help with this. 3.2 Processing 3.2.1 For Loops Never grow a vector, preallocate the size of objects used to store results. out &lt;- integer(50) for (i in 1:50) { out[i] &lt;- rpois(1, 10) } Use descriptive indices in loops to make them easier to understand. # First attempt for (i in 1:ncol(geno)) { for (j in 1:nrow(geno)) { output[i,j] &lt;- do_something(geno[j, i]) } } # Second attempt n_markers &lt;- ncol(geno) n_ind &lt;- nrow(geno) for (marker_ix in 1:n_markers) { for (individual_ix in 1:n_ind) { output[individual_ix, marker_ix] &lt;- do_something(geno[individual_ix, marker_ix]) } } If you really need to a grow a vector: Parallel Processing [need to do some research on this, see future package]. Rcpp - loops are faster using this. 3.2.2 Avoiding Loops Avoid loops if you can, use functional programming, e.g. purrr::map(). Note: loops are not necessarily slower but leave more room for the user to write them inefficiently. 3.2.3 Finding Slow Parts of Code Profiling code - e.g. using profviz (now built into the RStudio IDE) Benchmarking - e.g. with the bench package or the microbenchmark package. 3.3 Importing and Exporting Data For critical analyses, I still find the base R functions (e.g. read.csv()) to be most reliable but readr::read_csv() is much faster. vroom - package using the ALTREP framework, potentially extremely fast package for reading delimited files. writexl - the fastest package for writing to Excel and has no dependencies. 3.4 Further Resources Efficient R Programming by Colin Gillespie and Robin Lovelace: This book goes into great depth on working efficiently in R. How to speed up R code: an introduction: This resource is a little old (from 2015) but still contains some useful background on how to speed up R. "]
]
