[
["index.html", "R Training Resources and Tips Introduction", " R Training Resources and Tips Alan Yeung 11 June 2019 Introduction This is a list of R resources, links and tips that is being curated to help particularly with training on the Tidyverse and working efficiently in R. A good place to get started with learning about R is the PHI Introduction to R in NSS. If you want to try out R without installing it, you can do this on RStudio Cloud. For the tidyverse section, I provide a recommended order to work through materials for training. However, it is, of course, not necessary to follow this exactly – particularly if you feel you are already comfortable with some of the material. I leave it to the reader/participant to use this as they see fit. I am very grateful to Sean Kross for providing a simple-to-use bookdown template to get me started, and of course, Yihui Xie for creating the fantastic bookdown package. Finally, this is a work-in-progress and will need updating over time. Therefore, if you have suggestions, comments or feedback, please get in touch. "],
["getting-started.html", "Chapter 1 Getting Started 1.1 Understanding R from the Perspective of Excel 1.2 Setting Up RStudio 1.3 Getting Help", " Chapter 1 Getting Started 1.1 Understanding R from the Perspective of Excel This may be useful reading if you have never used R (or similar software for data analysis) before but are familiar with Microsoft Excel. R for Excel users Where do things live in R? R for Excel Users What’s a variable in R? 1.2 Setting Up RStudio When you first load RStudio, you should set up some options within it. First go to the Tools menu and then go into the Global Options. In the General tab, you should untick ‘Restore .RData into workspace at startup’ and change ‘Save workspace to .RData on exit’ to ‘Never’. This is illustrated in the Workflow: projects chapter in the R for Data Science book. Doing this helps to ensure that R loads in a fresh session every time which also helps to ensure your work is fully reproducible. In addition to this, you should make sure R is set up to work with the unicode character set in case you ever need to use them. To do this, go to the Code tab (within Global Options) and then go to the Saving tab within there and make sure the ‘default text encoding’ is ‘UTF-8’. Some optional settings: A lot of programmers like to use a dark theme for coding so if this is your preference, go to the Appearance tab and set the ‘Editor theme’ to something like ‘Twilight’. If you are working in the UK, go the Spelling tab, and set the ‘Main dictionary language’ to ‘English (United Kingdom)’. 1.3 Getting Help To use R effectively, it is more or less essential to learn how to use the help facilities within R. This is because there are just so many functions available (especially when you consider the amount of packages that have been developed to extend the capabilities of R) and it is difficult to always remember exactly how to use them. Help pages for functions in R can be brought up by typing ?name-of-function within R, e.g. to get the help for the mean() function, you can type ?mean. Equally, help(name-of-function) also works, e.g. help(mean). The help pages in R can be a little daunting at first so it is good to start by understanding the layout of a help page and what everything in it means. How to read a help page in R In addition to the help pages within R, there are also many other ways to get help. Where to get help with your R question? You can search online for help in your favourite search engine. Often typing something like ‘joining data in R’ will be good enough to return useful results. There is also a search page focussed on returning R-related results called RSeek if you want to try that. People are usually good at responding to questions if you post them online in a reproducible manner. Here is a good guide for doing this well using the reprex package in R. So you’ve been asked to make a reprex You can post your question to the Data Science Scotland Slack R Channel or to websites such as StackOverflow. "],
["tidyverse.html", "Chapter 2 Tidyverse 2.1 Installation 2.2 Getting Data into R 2.3 Data Manipulation with dplyr 2.4 Reshaping Data with tidyr 2.5 Other Data Manipulation Tips 2.6 Further Resources", " Chapter 2 Tidyverse This statement from the official tidyverse website describes what the tidyverse is The tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures. Further background information on what the tidyverse is can be found in the R Views blog: What is the tidyverse?. In addition, Hadley Wickham has laid out his principles for tidyverse packages within his tidy tools manifesto. At this point, you should watch this video to get an introduction to data wrangling with the tidyverse. What is data wrangling? Intro, Motivation, Outline, Setup – Pt. 1 Data Wrangling Introduction 2.1 Installation The package can be installed and loaded with install.packages(&quot;tidyverse&quot;) library(tidyverse) This loads the core set of tidyverse packages – see here for more details. 2.2 Getting Data into R The tidyverse installs a number of packages that help with getting your data into R. However, only the readr package is loaded as part of the core set of packages so the other data import packages, (readxl and haven have to be loaded with their own library() calls). readr: for reading in data formats such as csv, tsv and fwf. readxl: for reading in Excel data formats (.xls and .xlsx). haven: for reading in SPSS, Stata and SAS data formats. RStudio also provides an ‘Import Dataset’ button to help with this. The RStudio support pages describe how to import data with RStudio. If you decide to use this to help you import data into R, my recommendation is to copy over the code generated in the ‘Code Preview’ box into your R script. This helps you to keep a record of the data import step in your script. Besides these data types, you should familiarise yourself with R’s own data formats: .rds and .RData – .rds format can be used when saving a single object or dataset while .RData should only be used when you want to save multiple objects or datasets from your R session. These formats load fast into R and an important advantage is that they keep data exactly as they were when they were created in R (i.e. all information about variable types and metadata will be retained after saving). .rds: read in data with readRDS(); save data with saveRDS() .RData: read in data with load(); save data with save() 2.3 Data Manipulation with dplyr dplyr provides a set of functions (often described as verbs) to help with data manipulation. Most common tasks can be done using only a handful of functions: select(), filter(), mutate(), rename(), arrange(), summarise() and group_by() – note that rename() is not usually included in this list but I have included it as renaming variables comes up a lot during data cleaning/processing. Out of these functions, group_by() is the most complex and should probably be tackled after understanding the others; in particular, it is often used together with summarise(). Furthermore, two functions that often come in handy when used together with mutate() are ifelse() (or the stricter dplyr equivalent if_else()) and case_when() – these help to create new variables depending on certain conditions being met. It is a good idea to understand what each of these functions does on their own first before thinking about how to combine multiple manipulation steps together using pipes (%&gt;%). Once you are comfortable with each of these functions, it should be a relatively small step to learn how to combine them together with the pipe operator; the ability to do this in a way that is relatively intuitive is one of the most powerful design features of the tidyverse. This is more or less the learning order used in the dplyr chapter of the R Programming for Data Science book which is worth a read through (there is also an accompanying video for that chapter which may also help). I would definitely recommend spending a good amount of time learning dplyr as you will want to reach the point where you can use these functions without requiring much thinking. A good place to get started on learning about these functions are these videos. Data Manipulation Tools: dplyr – Pt 3 Intro to the Grammar of Data Manipulation with R Hands-on dplyr tutorial for faster data manipulation in R – note that this tutorial is a little old (2014) so some parts of it are out-of-date but I think it is still useful. Only minor changes will be needed to make this current and the old dplyr functions mentioned should still be useable anyway. For example, they use tbl_df() which can now be replaced by tibble() instead. You should try running the R commands yourself while watching this video to aid the learning process. Once you have watched these videos, you can work through all the tutorials in the Work with Data section on RStudio Cloud. Then you can further consolidate this knowledge by reading through data transformation chapter of the R for Data Science book and working through the exercises at the end. After this, a set of four tutorials by Suzan Baert provides much more depth on what some of the basic dplyr functions can do. Again, trying to run the R codes while following along with the tutorials will be very beneficial. Data Wrangling Part 1: Basic to Advanced Ways to Select Columns Data Wrangling Part 2: Transforming your columns into the right shape Data Wrangling Part 3: Basic and more advanced ways to filter rows Data Wrangling Part 4: Summarizing and slicing your data 2.3.1 Working with Multiple Datasets It is very likely that you will need to perform data linkage at some point so this makes it more or less essential to learn how to work with the various joining functions within dplyr. To get a quick overview on this, you can watch this short video. Working with Two Datasets: Binds, Set Operations, and Joins – Pt 4 Intro to Data Manipulation After this short introduction, I recommend reading three short sections of the relational data chapter in the R for Data Science book: Understanding joins, Inner join, Outer joins. Then you can follow this video tutorial – the parts about joins start from around 25 minutes into the video but if you want to further solidify your dplyr knowledge, you can watch it from the beginning. Again, note that this video is a little old (2015) so some aspects may be out-of-date but it is still useful nevertheless. Going deeper with dplyr: New features in 0.3 and 0.4 (tutorial) There is also a Join Data Sets tutorial on RStudio Cloud that you can try. Finally, you will want to read and work through all of relational data chapter in the R for Data Science book to understand how to work with multiple related datasets more thoroughly. By the end, you should be familiar with just about everything on this Data Transformation Cheat Sheet. 2.4 Reshaping Data with tidyr Sometimes you will need to format your data into a specific shape to make it work for a certain purpose – tidyr is a package that has functions that help with this. For example, for output tables, you may want to have data on different years in separate columns while for analytical operations within R, it may be easier to work with ‘year’ stored in only one column (or variable); the latter format here can be considered as tidy data. In particular, the ggplot2 package often requires data to be in the long or tidy format. To get a better idea on this, watch this video. Tidy Data and tidyr – Pt 2 Intro to Data Wrangling with R and the Tidyverse – however please note that the gather() and spread() functions mentioned will be replaced by pivoting functions so you should mainly be concerned with the concepts here rather than learning about these functions specifically. Then you can try the Reshape Data tutorial on RStudio Cloud. The main thing you need to be able to do with tidyr is to be confident in knowing how to transform your data from wide-to-long format or from long-to-wide format. However, if you want to know more about what tidyr can do or find out more about tidy data in general, please refer to the tidy data chapter in the R for Data Science book (keeping in mind the caveat made previously about the change to using pivoting functions). 2.5 Other Data Manipulation Tips As you progress further along with learning to manipulate data using the tidyverse, you will inevitably encounter situations where you need to know more about how to deal with specific types of data such as dates/times and string variables. In addition, there will also be times when you wish there was a convenient and quick way to process data – luckily there usually is in R. Please note that some of the material mentioned in this section is probably optional until you specifically have a need for it but I recommend that you go through some of it anyway to build an awareness of where to look for solutions when you need it. 2.5.1 Dates/Times The main package within the tidyverse for dealing with dates and times is the lubridate package. This is not loaded with the core set of tidyverse packages so has to be loaded separately by typing library(lubridate). The main reference text for learning about how to work with dates/times using lubridate is the dates and times chapter in R for Data Science. Dates/times can sometimes be very complex (e.g. when thinking about time calculations across timezones) but it is likely that you will usually only need to perform fairly simple data processing operations involving dates. In this case, you will only really need to know a small number of functions from lubridate: dmy(), mdy(), ymd() to convert text to a Date format in R, e.g. ymd(\"2001/05/20\") would convert the text “2001/05/20” so that R can recognise this as 20 May 2001. year(), month(), day() to extract the year, month, day from a date object, e.g. year(ymd(\"2001/05/20\")) would extract 2001. 2.5.2 Strings The stringr package package is loaded within the core set of tidyverse packages and provides a set of consistent functions for working with string (or text) data. Again, the main reference is the strings chapter in R for Data Science. You will likely not need to know everything contained in that chapter but I would recommend learning some basics of using regular expressions. These can help to, for instance, filter data or create new variables according to some text pattern found in a string variable. This can sometimes be much quicker than filtering using a list of keywords, particularly if your data may contain variations on a common item (e.g. ‘Male’, ‘M’, ‘Man’) or mispellings (e.g. ‘Mal’). A useful place to practice and learn how to use regular expressions is regexr. There is also an RStudio addin that may help called regexplain which was inspired by regexr but may help with your learning as it can be used within RStudio. Once you gain some familiarity on how to use regular expressions, many of the stringr functions should be relatively easy to understand and use. In particular, str_detect() is useful for filtering and creating variables. If you are already familiar with regular expressions using base R functions and want to find out about their stringr equivalents to take advantage of the consistencies in their design, the from base R to stringr vignette is a handy resource. Furthermore, this blog post on Demystifying Regular Expressions in R gives a very clear and descriptive guide to how the different base R regular expression functions work. 2.5.3 janitor clean_names() tabyl() 2.6 Further Resources R for Data Science by Garrett Grolemund and Hadley Wickham R for Data Science: Exercise Solutions by Jeffrey B. Arnold "],
["efficiency.html", "Chapter 3 Efficiency 3.1 Main Resources 3.2 Processing 3.3 Memory Management 3.4 Importing and Exporting Data", " Chapter 3 Efficiency 3.1 Main Resources Efficient R Programming by Colin Gillespie and Robin Lovelace: This book goes into great depth on working efficiently in R. How to speed up R code: an introduction: This resource is a little old (from 2015) but still contains some useful background on how to speed up R. 3.2 Processing 3.2.1 For Loops Never grow a vector, preallocate the size of objects used to store results. out &lt;- integer(50) for (i in 1:50) { out[i] &lt;- rpois(1, 10) } Use descriptive indices in loops to make them easier to understand. # First attempt for (i in 1:ncol(geno)) { for (j in 1:nrow(geno)) { output[i,j] &lt;- do_something(geno[j, i]) } } # Second attempt n_markers &lt;- ncol(geno) n_ind &lt;- nrow(geno) for (marker_ix in 1:n_markers) { for (individual_ix in 1:n_ind) { output[individual_ix, marker_ix] &lt;- do_something(geno[individual_ix, marker_ix]) } } If you really need to a grow a vector: Parallel Processing [need to do some research on this, see future package]. Rcpp - loops are faster using this. 3.2.2 Avoiding Loops Avoid loops if you can, use functional programming, e.g. purrr::map(). Note: loops are not necessarily slower but leave more room for the user to write them inefficiently. 3.2.3 Finding Slow Parts of Code Profiling code - e.g. using profviz (now built into the RStudio IDE) Benchmarking - e.g. with the bench package or the microbenchmark package. 3.3 Memory Management 3.3.1 Efficiently Extracting Data from Databases Extract only data necessary for analysis from databases [reference Emily Moore’s SMRA paper detailing efficient SQL queries]. If you can use the odbc package to connect to databases, you can use dbplyr to write efficient SQL queries using dplyr code. Once you are sure your query will extract what you need (view generated SQL code from dplyr using show_query()), the data can be downloaded into R using collect(). 3.3.2 Temporary Objects Remove temporary objects as you go along in an analysis, particularly if those objects take up a lot of memory (e.g. temporary large dataframes). The objectremover RStudio addin may help with this. 3.3.3 Caching memoise - more advanced. 3.4 Importing and Exporting Data For critical analyses, I still find the base R functions (e.g. read.csv()) to be most reliable but readr::read_csv() is much faster. vroom - experimental package using the ALTREP framework but potentially extremely fast package for reading delimited files. writexl - the fastest package for writing to Excel and has no dependencies. "]
]
